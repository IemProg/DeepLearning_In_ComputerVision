{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Recognition task.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fKw19Zvbex8",
        "colab_type": "text"
      },
      "source": [
        "# Face recognition using neural network features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyfXUqdFbex_",
        "colab_type": "text"
      },
      "source": [
        "In this task, you have to construct face recognizer based on features extracted from the neural network. The task consists of two parts: image classification and video classification. In the first one you should classify distinct images and in the second one you will deal with short video sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Oxi1-gbeyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b473f26d-d729-41b9-c511-dfe761dbe663"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Flatten, Dense, Activation\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras import backend as K"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pYgPq9nEbeyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9dac2c6-8fbf-4b56-b8db-b0e7e9e8a351"
      },
      "source": [
        "%pylab inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from copy import copy\n",
        "from collections import Counter"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubcpinvqbeyc",
        "colab_type": "text"
      },
      "source": [
        "First of all, you have you have to read the data. Run the cell below to unpack data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOWGcnHlbl_q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0737f689-ac3d-4b89-e56a-8a01a025cfaa"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOk1hgyAbuKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfe66bdd-abf7-40f2-8247-6df8f09107f0"
      },
      "source": [
        "cd /content/gdrive/My Drive/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifjaQHpib1_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cc05cfe-ffe7-4bbd-e177-442ac2826a87"
      },
      "source": [
        "cd face-recognition-task/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/face-recognition-task\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-hzV9Tubeyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from get_data import unpack\n",
        "unpack('Face_Recognition_data.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHksF3txb0JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DxMjGdNbeyh",
        "colab_type": "text"
      },
      "source": [
        "### Reading data for image and video classification (1 point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgrLxcpNbeyi",
        "colab_type": "text"
      },
      "source": [
        "Implement function $\\tt{load}$\\_$\\tt{image}$\\_$\\tt{data}$. It should return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing ones. The keys of the dictionaries are the names of the image files and the values are 3-dimensional numpy arrays (for images) or strings with the names (for labels).\n",
        "\n",
        "$\\tt{dir}$\\_$\\tt{name}$ is the name of directory with data for image classification. If 'Face_Recofnition_data' directory is located in the same directory as this notebook, then the default value can be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzvLf_kPbeyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from skimage.io import imread\n",
        "def load_image_data(dir_name = 'Face_Recognition_data/image_classification'):\n",
        "    x_train, y_train, x_test, y_test = {}, {}, {}, {}\n",
        "    \n",
        "    #Training dataset\n",
        "    for filename in os.listdir(os.path.join(dir_name, \"train\", \"images\")):\n",
        "      img = imread(os.path.join(dir_name, \"train\", \"images\", filename))\n",
        "      if img.shape == 2:\n",
        "        img = gray2rgb(img)\n",
        "      x_train[filename] = img\n",
        "    \n",
        "    df_train = pd.read_csv(os.path.join(dir_name, \"train\", \"y_train.csv\"))\n",
        "    for filename, class_id in zip(df_train[\"filename\"], df_train[\"class_id\"]):\n",
        "      y_train['filename'] = class_id \n",
        "    \n",
        "    #Testing dataset\n",
        "    for filename in os.listdir(os.path.join(dir_name, \"test\", \"images\")):\n",
        "      img = imread(os.path.join(dir_name, \"test\", \"images\", filename))\n",
        "      if img.shape == 2:\n",
        "        img = gray2rgb(img)\n",
        "      x_test[filename] = img\n",
        "    \n",
        "    df_test = pd.read_csv(os.path.join(dir_name, \"test\", \"y_test.csv\"))\n",
        "    for filename, class_id in zip(df_test[\"filename\"], df_test[\"class_id\"]):\n",
        "      y_test['filename'] = class_id \n",
        "    \n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xhzc-TUkbeyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d08a1be8-6652-4eea-e112-7cd2bdfa358a"
      },
      "source": [
        "x_train, y_train, x_test, y_test = load_image_data()\n",
        "print '%d'%len(x_train), '\\ttraining images'\n",
        "print '%d'%len(x_test), '\\ttesting images'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "661 \ttraining images\n",
            "808 \ttesting images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydZkQ1eXbeyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize(data, labels, function = lambda x:x[0], n_cols = 5, n_rows=1):\n",
        "    figure(figsize = (3*n_cols,3*n_rows))\n",
        "    for n,i in enumerate(np.random.choice(data.keys(), size = n_cols*n_rows)):\n",
        "        plt.subplot(n_rows,n_cols,n+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(function([data[i]]))\n",
        "        plt.title(labels[i])\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjwzNFgGbey0",
        "colab_type": "text"
      },
      "source": [
        "That is how the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqL49wZabey1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize(x_train, y_train)\n",
        "visualize(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJATRRYVbezB",
        "colab_type": "text"
      },
      "source": [
        "Let us now read the video classification data, as well. You have to implement function to load video data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXQaqMILbezD",
        "colab_type": "text"
      },
      "source": [
        "Function $\\tt{load}$\\_$\\tt{video}$\\_$\\tt{data}$ should also return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing videos and labels. The training data is in the same format as in image classification task. The keys of testing data and labels are video ids and the values are the lists of frames and the strings with names, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHGIJTbtbezF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from skimage.io import imread\n",
        "def load_video_data(dir_name = 'Face_Recognition_data/video_classification'):\n",
        "    video_train, train_labels, video_test, test_labels = {},{},{},{}\n",
        "    \n",
        "    #Training dataset\n",
        "    for filename in os.listdir(os.path.join(dir_name, \"train\", \"images\")):\n",
        "      img = imread(os.path.join(dir_name, \"train\", \"images\", filename))\n",
        "      if img.shape == 2:\n",
        "        img = gray2rgb(img)\n",
        "      video_train[filename] = img\n",
        "    df_frame = pd.read_csv(os.path.join(dir_name, \"train\", \"y_train.csv\"))\n",
        "    for filename, class_id in zip(df_frame[filename], df_frame[class_id]):\n",
        "        train_labels[filename] = class_id\n",
        "    \n",
        "    #Testing dataset\n",
        "    for video_name in os.listdir(os.path.join(dir_name, \"test\", \"videos\")):\n",
        "      video_test[video_name] = []\n",
        "      for filename in os.path.join(dir_name, \"test\", \"videos\", video_name):\n",
        "        img = imread(os.path.join(dir_name, \"test\", \"videos\", video_name, filename))\n",
        "        if img.shape == 2:\n",
        "          img = gray2rgb(img)\n",
        "        video_test[video_name].append(img)\n",
        "        \n",
        "    df_frame = pd.read_csv(join(dir_name, \"test\", \"y_test.csv\"))\n",
        "    for filename, class_id in zip(df_frame[filename], df_frame[class_id]):\n",
        "        train_labels[str(filename)] = class_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4RKlNkjbezL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_train, train_labels, video_test, test_labels = load_video_data()\n",
        "print '%d'%len(video_train), '\\ttraining images'\n",
        "print '%d'%len(video_test), '\\ttesting videos'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afYQIScIbezQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize(video_train, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS8NsqSLbezX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize({i:video_test[i][1] for i in video_test}, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyT5K4Etbezf",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing (3 points)\n",
        "You have to implement preprocessing function in the cell below.\n",
        "Getting a list of images as an input the this function should detect the face on each image, find the facial keypoints () and then crop and normalize the image according to these keypoints. The output of this function is the list of images which contain only the aligned face and should be converted to the tensor of the shape $(N, 224, 224, 3)\\ $ where $N$ is the length of the initial list. You can add extra arguments to the preprocess function if necessary (i.e. flag $\\tt{is}$\\_$\\tt{video}$ to determine if the list of images is video sequence or not).\n",
        "\n",
        "For face detection and facial keypoint regression you can use your models from the previous tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ZgiZnmbezj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dlib\n",
        "from cv2 import resize\n",
        "\n",
        "def preprocess_imgs(imgs):\n",
        "  #Using already built-in detector from dlib \n",
        "  detector = dlib.get_frontal_face_detector()\n",
        "  output = []\n",
        "  \n",
        "  for image in imgs:\n",
        "    detected = detector(image)\n",
        "    if len(detected) == 0:\n",
        "      #Lets resize the image\n",
        "      cropped = resize(img, (224, 224))\n",
        "      output.append(output)  \n",
        "    for item in detected:\n",
        "      top = item.top()\n",
        "      bottom = item.bottom()\n",
        "      \n",
        "      left = item.left()\n",
        "      right = item.right()\n",
        "      height = top - bottom\n",
        "      width = right - left\n",
        "      \n",
        "      scale = 0.2\n",
        "      top = max(0, int(top - height*scale))\n",
        "      bottom = min(img.shape[0], int(bottom + height*scale))\n",
        "      left = max(0, int(left - width*scale))\n",
        "      right = min(img.shape[1], int(right + width*scale))\n",
        "      \n",
        "      if top >= 0 and left >= 0:\n",
        "        cropped = img[top:bottom, left:right, :]\n",
        "        cropped = cv2.resize(cropped, (224, 224))\n",
        "        output.append(cropped)\n",
        "        break\n",
        "        \n",
        "  if len(res) == 0:\n",
        "    imshow(imgs[0])\n",
        "    \n",
        "  return output  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ_zlMqbezq",
        "colab_type": "text"
      },
      "source": [
        "#### Visualization of preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNTKdaqabezr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize(x_train, y_train, function = lambda x:preprocess_imgs(x)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yngshadgbez5",
        "colab_type": "text"
      },
      "source": [
        "The neural network is already trained on the other face dataset. You should use this network as feature extractor to get descriptors of the faces. You can choose any hidden layer you need (or several layers) to extract features and any classification method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvUH6z9Mbez-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1261
        },
        "outputId": "903e2477-ef0c-4d7e-96fa-023670af25b5"
      },
      "source": [
        "import h5py\n",
        "from keras.models import load_model\n",
        "model = load_model('face_recognition_model.h5')\n",
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0620 20:51:54.997370 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0620 20:51:55.048403 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0620 20:51:55.061873 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0620 20:51:55.128289 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0620 20:52:05.263159 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0620 20:52:05.264493 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0620 20:52:08.149512 140243776857984 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc6 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc6/relu (Activation)        (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "fc7 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "fc7/relu (Activation)        (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "fc8 (Dense)                  (None, 500)               2048500   \n",
            "_________________________________________________________________\n",
            "prob (Activation)            (None, 500)               0         \n",
            "=================================================================\n",
            "Total params: 136,309,044\n",
            "Trainable params: 136,309,044\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wePPEv7wbe0H",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of using the network as feature extractor. The shape of input tensor has to be (n_images, 224, 224, 3), so you can input several images simultaneously and get their face descriptors of shape (n_images, n_components)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWDPwPeJbe0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_layer_output(images, layer = 'fc6'):\n",
        "    assert len(images.shape)==4, 'Wrong input dimentionality!'\n",
        "    assert images.shape[1:]==(224,224,3), 'Wrong input shape!'\n",
        "    \n",
        "    network_output = model.get_layer(layer).output\n",
        "    feature_extraction_model = Model(model.input, network_output)\n",
        "    \n",
        "    output = feature_extraction_model.predict(images)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWvwJak_be0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ca52978-bbb6-4121-bedd-abd2cfe5ce07"
      },
      "source": [
        "img = cv2.resize(x_train['0.jpg'], (224,224)).reshape(1,224,224,3)\n",
        "out = get_layer_output(img)\n",
        "print out.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2v9jSMbe0U",
        "colab_type": "text"
      },
      "source": [
        "### Training classifier (2 points)\n",
        "\n",
        "\n",
        "You have to implement class $\\tt{Classifier}$ with methods $\\tt{fit}$, $\\tt{classify}$\\_$\\tt{images}$ and $\\tt{classify}$\\_$\\tt{videos}$ in the cell below. \n",
        "The method $\\tt{Classifier.fit}$ gets two dictionaries as input: train images and labels, and trains the classifier to predict the person shown on the image.\n",
        "$\\tt{Classifier.classify}$\\_$\\tt{images}$ gets the dictionary of test images (with filenames as keys) as input and should return the dictionary of the predicted labels.\n",
        "$\\tt{Classifier.classify}$\\_$\\tt{videos}$ is similar to the previous one, but gets the dictionary of test videos (with video as keys) as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzoi14bube0W",
        "colab_type": "text"
      },
      "source": [
        "To classify video you can combine the predictions for its frames any way you want (averaging, voting, etc.).\n",
        "If video classification takes too long you can use face detector not in all the frames but every few frames while preprocessing video frames. \n",
        "Besides, sometimes the face is hardly detected on the image and the frame in which the detector works wrong can add noise to the prediction. Hence, the result of the prediction without using such frames may be better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whcBp5aZbe0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
        "from skimage.io import imread\n",
        "import cv2\n",
        "from os.path import join\n",
        "\n",
        "class Classifier():\n",
        "    def __init__(self, nn_model, layer='fc7'):\n",
        "        network_output = nn_model.get_layer(layer).output\n",
        "        self.feature_extraction_model = Model(nn_model.input, network_output)\n",
        "        self.clf = kNN(n_neighbors=5, weights='distance')\n",
        "\n",
        "\n",
        "    def fit(self, train_imgs, train_labels):\n",
        "        train_X = []\n",
        "        train_y = []\n",
        "        for k, v in train_imgs.items():\n",
        "            pre_img = preprocess_imgs([v])[0]\n",
        "            pre_img = pre_img[np.newaxis, ...]\n",
        "            train_X_features = self.feature_extraction_model.predict(pre_img)\n",
        "            train_X.append(train_X_features)\n",
        "            train_y.append(train_labels[k])\n",
        "            \n",
        "        train_X = np.asarray(train_X)\n",
        "        train_X = np.reshape(train_X, (-1, 4096))\n",
        "        train_y = np.asarray(train_y)\n",
        "        self.clf.fit(train_X,train_y)\n",
        "\n",
        "\n",
        "    def classify_images(self, test_imgs):\n",
        "        res = {}\n",
        "        for k, v in test_imgs.items():\n",
        "            pre_img = preprocess_imgs([v])[0]\n",
        "            pre_img = pre_img[np.newaxis, ...]\n",
        "            test_img_features = self.feature_extraction_model.predict(pre_img)\n",
        "            predicted_y = self.clf.predict(test_img_features)[0]\n",
        "            res[k] = predicted_y\n",
        "       \n",
        "        return res\n",
        "       \n",
        "        \n",
        "    def classify_videos(self, test_video):\n",
        "        res = {}\n",
        "        for k, v in test_video.items():\n",
        "            table = Counter()\n",
        "            for img in v:\n",
        "                pre_img = preprocess_imgs([img])[0]\n",
        "                pre_img = pre_img[np.newaxis, ...]\n",
        "                test_img_features = self.feature_extraction_model.predict(pre_img)\n",
        "                predicted_y = self.clf.predict(test_img_features)[0]\n",
        "                table[predicted_y] += 1\n",
        "            res[k] = max(table, key=lambda x:table[x])\n",
        "\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpbv0Iybbe0b",
        "colab_type": "text"
      },
      "source": [
        "Now we can build the classifier, fit it and use to predict the labels of testing images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfQCd5ywbe0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_classifier = Classifier(model)\n",
        "img_classifier.fit(x_train, y_train)\n",
        "y_out = img_classifier.classify_images(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxzbQa8Rbe0i",
        "colab_type": "text"
      },
      "source": [
        "### Image classification quality (2 points)\n",
        "\n",
        "Let us check the accuracy of your classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.90, to obtain 2 points — at least 0.95."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GRRP35Mbe0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_test(output, gt):\n",
        "    correct = 0.\n",
        "    total = len(gt)\n",
        "    for k, v in gt.items():\n",
        "        if output[k] == v:\n",
        "            correct += 1\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return 'Classification accuracy is %.4f' % accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eGVYPqywsOHX",
        "colab": {}
      },
      "source": [
        "print check_test(y_out, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToKtUz50be0v",
        "colab_type": "text"
      },
      "source": [
        "### Video classification quality (2 points)\n",
        "\n",
        "Let us check the quality of video classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.80, to obtain 2 points — at least 0.85."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxV6IijUbe0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video_classifier = Classifier(model)\n",
        "video_classifier.fit(video_train, train_labels)\n",
        "y_video_out = video_classifier.classify_videos(video_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXiC5KYNbe0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print check_test(y_video_out, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcyXXMMfbe03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}